+++
author = "FlintyLemming"
title = "Deepseek R1 推理需求"
slug = "1977bda595c58157b48bf45cc8239758"
date = "2025-02-11"
description = "没钱别玩"
categories = ["HomeLab"]
tags = ["Deepseek"]
image = "https://hf-image.mitsea.com:8840/blog/posts/2025/02/Deepseek%20R1%20%E6%8E%A8%E7%90%86%E9%9C%80%E6%B1%82/spenser-sembrat-QLYH_8w6PKs-unsplash.avif"
+++

## 说明

1. 前面的所有内容都不考虑量化，量化放在最后低开销方案那一节里
2. 不考虑 fp8 动态转换，问的人都不推荐，性能损失大，所以后面一律默认没有 fp8 计算单元的卡就跑不了 fp8 模型
3. Deepseek 是原生 fp8 模型，所以模型显存占用就约等于参数量（671B 的 617，实际大概 700GB），不需要再乘 2，但是需要推理设备有 fp8 的计算单元
4. 除了模型外，还要保留上下文和并发需要的 KV Cache，大概 2、300GB，并发需要的 KV Cache 无上限。所以对于有 fp8 计算单元的设备，显存至少要 1T 显存；没有 fp8 只有 bf16 的，模型占用显存就要翻倍，就是 1.4T，上下文其实也要翻倍，就不算了，大概就是 1.6T 显存以上。所以 A100 不是很推荐。
5. 模型推理不吃卡间互联带宽，甚至 Fine-tune 都不怎么吃，只有正儿八经训练才吃
6. GB200 没写在方案里，太贵、货少、硬件设计散热似乎有问题

## 看法

1. 显卡迭代速度太快，A100 在 Deepseek 面前由于没有 fp8 计算单元，可以说已经被淘汰。Blackwell 又推出了原生 fp4 计算单元，如果接下来有很强的开源 fp4 模型，那 H100 H200 可以说又是被淘汰。
2. 模型推理不需要通用计算，它只需要显卡某一种、两种的计算单元。不排除未来几年会有推理专用硬件
3. 列出的配置也就是能跑，并发大概也就够小几十人用，上一点规模就自行再翻倍
4. 上面三点总结起来就是很费钱，有实力再搞

## 要点区分

### 推理硬件类型/厂家不同

|  | 纯 CPU 推理 | Nvidia 显卡 | AMD 显卡 | 华为显卡 |
| --- | --- | --- | --- | --- |
| Pros | 成本最低 | 性能最好，而且结果可预测（不需要怎么折腾就可以用起来） | 价格比 Nvidia 平台稍低，性能良好，官方表明支持 Deepseek 推理 | （还不是很了解） 硅基流动说他家用华为的卡做的推理，感觉工具链应该是能跑起来 |
| Cons | 速度太慢，而且并发性能衰减大 | 价格高 | AMD 驱动等各种工具链还在频繁更新优化性能，可能会碰到来自上游的问题 | 资料少，很难获得支持，可能要折腾，而且购买渠道不熟悉 |

### Nvidia 显卡代际不同

|  | Pascal | Volta | Turing | Ampere | Ada | Hopper | Blackwell |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 发布时间 | 2016 | 2017 | 2018 | 2020 | 2022 | 2022 | 2024 |
| 典型显卡 | Tesla P40、GeForce GTX 1080 | Tesla V100 | Quadro RTX 6000、RTX 2080 | RTX A6000（现在 GPU）服务器用的、A100、RTX 3090 | RTX 6000 Ada、L40、RTX 4090 | H100、H200 | B200、RTX 5090 |

### Nvidia 显卡接口不同

有 PCIe 和 SXM 两种接口，后者必须搭配 SXM 接口的专用服务器，带宽大一点，性能强大概 19%，更适合训练。但是价格似乎差不太多。

## 具体方案

### 显卡方案

| 方案 | 自组双机 A100/A800 PCIe | 单台 DGX H200 | 自组单机8卡 H200 SXM  | 自组双机 H100/H800 SXM/PCIe | 国行 H40 SXM 双机 | AMD MI300x 整机 | 华为 910B/910C |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 态度 | 不推荐 | 最推荐 | 最推荐 | 推荐 | 不是很推荐 | 一般推荐 | 不是很推荐 |
| 概述 | 自行购买10卡服务器两台，再单独购买 20 块 A100（甚至感觉都不够） | 能买到的最新最快的 Nvidia 官方平台 | 最适中的方案 | 主流方案之一 | 好处就是全部都是正规渠道完整保修 | 有点灵车，属于是钱足够的话我是不想用，钱不够的话不是不能用 | 只能说有成功跑起来的案例，硅基流动说他们用的华为 |
| 优点 | 1. 没有 fp8 计算单元，最近抛货比较多，价格有所降低；2. 购买渠道比较灵活方便 | 1. 有原生 fp8 计算单元，比 A100 架构更新，推理速度快；2. 单机，不需要考虑双机互联的带宽瓶颈问题；3. Nvidia 整机方案可靠性比较好 | 1. 周边配置可以选低一点，价格会比 DGX H200 便宜；2. 单机，不需要考虑双机互联的带宽瓶颈问题 | 1. 有原生 fp8 计算单元，比 A100 架构更新，推理速度快；2. 购买渠道比较灵活方便 | 1. 未禁售，货源可靠，有完整保修；2. 好购买 | 1. 有还算比较稳定货源；2. 有成功跑起来的案例和测试数据；3. AMD 官方自己站台了是支持 Deepseek | 想不出来 |
| 缺点 | 1. Ampere 这代显卡没有 fp8 计算单元，如果跑不量化的版本，只能跑 scale 上去的 bf16，模型本身占用的显存就要翻倍（大概 1.4T），剩余给上下文和并发的 KV Cache 没多少；2. 卡已禁售，买到的原则上都是二手，只能依靠经销商保修；3. A100 架构放现在算比较老的了，两台纯粹是堆显存，就这可能还不太够 | 1. 货少，价格不透明，可能比较贵；2. 显卡使用专有接口连接至主板，非 PCIe 接口，不好升级和更换；3. 一体化程度比较高，又是走私产品，一旦发生意外硬件损坏感觉会比较难修（得问经销商有没有能力） | 1. 卡已禁售，买到的原则上都是二手，只能依靠经销商保修 | 1. 卡已禁售，买到的原则上都是二手，只能依靠经销商保修 | 1. H40 中国特供卡，而且连 Nvidia 官网都没 Datasheet；2. 虽然有 fp8 计算单元，但是性能被砍很多，具体可以看 补充说明 一节 | 1. AMD 的驱动比较草台，可能会有一些驱动导致的上游问题；2. ROCm 推理 AMD 官方是推荐用 SGLang，SGLang 本身还不是非常成熟，活跃更新中，跑 Deepseek 要不可避免使用测试版本 | 1. 资料很少，而且华为自己就不怎么喜欢对外提供资料；2. 910B 和 C 都不支持 fp8，显存占用也是很大 |
| 总显存 | 80G x 20 = 1600GB | 141G x 8 = 1128GB | 141G x 8 = 1128GB | 80G x 16 = 1280GB | 96G x 16 = 1536GB | 192G x 8 = 1536GB | - |
| 电源 | 约 10 kW | 约 8 kW | 约 8 kW | 约 10 kW | - | - | - |
| 官方链接 | - | [Link](https://docs.nvidia.com/dgx/dgxh100-user-guide/introduction-to-dgxh100.html) | - | - | - | - | - |
| 价格 | 200w 左右（大概估的，现在感觉有点虚高，不支持 fp8 还得掉价） | 285w 左右（闲鱼询问价格） | 245w 左右（闲鱼询价格） | 大概也就差不多价格 | 220w 左右（Dell X E9680）（经销商价格） | 200w 左右 （朋友朋友圈价格） | - |

## 廉价方案

### CPU

不推荐，并发很差，只是内存多的情况下验证模型还行，没法实际多人用

### 量化模型

unsloth 出了动态量化的版本

![](https://hf-image.mitsea.com:8840/blog/posts/2025/02/Deepseek%20R1%20%E6%8E%A8%E7%90%86%E9%9C%80%E6%B1%82/image.avif)

disk size 就约等于显存占用量，说是 2.51-bit 效果还不错，但是没有条件验证。可以考虑在现有机器上加点内存验证，或者等有没有人提供这个模型的 API。

但是他官方用的 llama.cpp 跑的，这个框架气氛上好像没什么多人并发的性能，还没看到用 vLLM 跑的实际测试数据。

[https://unsloth.ai/blog/deepseekr1-dynamic](https://unsloth.ai/blog/deepseekr1-dynamic)

## 补充说明

### 华为 910B 相关实例

[https://zhuanlan.zhihu.com/p/22564788535](https://zhuanlan.zhihu.com/p/22564788535)

### 配置详细

主要就是卡的钱，周边都是小头了

自组单机8卡 H200 SXM 那个配置如下

![](https://hf-image.mitsea.com:8840/blog/posts/2025/02/Deepseek%20R1%20%E6%8E%A8%E7%90%86%E9%9C%80%E6%B1%82/1.avif)

国行 H40 SXM 双机那个配置如下

6542Y*2/1T DDR5内存/4块7.68T NVME/8个H20-96G/Boss卡含2块480G SSD/2千兆2万兆/6个2800W电源

### 关于 H20

fp8 砍到姥姥家了，性能只有 H100 的 14%，感觉是诈骗产品.jpg

而且这个卡的数据 Nvidia 官网查不到，非常神秘

除了国内正规供货，想不出任何理由买它

![](https://hf-image.mitsea.com:8840/blog/posts/2025/02/Deepseek%20R1%20%E6%8E%A8%E7%90%86%E9%9C%80%E6%B1%82/image%201.avif)